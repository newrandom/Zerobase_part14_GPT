{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UVA_4MZ7xNwP"
      },
      "source": [
        "# Positional Encoding code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WeyDFwIGwqtt"
      },
      "outputs": [],
      "source": [
        "### Numpy version ###\n",
        "def positional_encoding(max_position, d_model, min_freq=1e-4):\n",
        "    position = np.arange(max_position)\n",
        "    freqs = min_freq**(2*(np.arange(d_model)//2)/d_model)\n",
        "    pos_enc = position.reshape(-1, 1)*freqs.reshape(1, -1)\n",
        "    pos_enc[:, ::2] = np.cos(pos_enc[:, ::2])   \n",
        "                # 짝수\n",
        "    pos_enc[:, 1::2] = np.sin(pos_enc[:, 1::2])\n",
        "                # 홀수\n",
        "    return pos_enc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j4mIWBDXxmXv"
      },
      "source": [
        "# Multi-head Self-attention"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bDYqTL5jxpkX"
      },
      "outputs": [],
      "source": [
        "# 선형대수는 배워야 한다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vdMj4Rs72G1r"
      },
      "outputs": [],
      "source": [
        "class MultiHeadAttentionLayer(nn.Module):\n",
        "    def __init__(self, hidden_dim, n_heads, dropout_ratio,device):\n",
        "        super().__init__()\n",
        "\n",
        "        assert hidden_dim % n_heads == 0\n",
        "\n",
        "        self.hidden_dim = hidden_dim    # 임베딩 차원\n",
        "        self.n_heads = n_heads  # 헤드(head)의 개수 : 서로 다른 어텐션(attention) 컨셉의 수\n",
        "        self.head_dim = hidden_dim // n_heads   # 각 헤드(head)에서의 임베딩 차원\n",
        "\n",
        "        self.fc_q = nn.Linear(hidden_dim, hidden_dim)   # Query 값에 적용될 FC 레이어\n",
        "        self.fc_k = nn.Linear(hidden_dim, hidden_dim)   # Key 값에 적용될 FC 레이어\n",
        "        self.fc_v = nn.Linear(hidden_dim, hidden_dim)   # Value 값에 적용될 FC 레이어\n",
        "\n",
        "        self.fc_o = nn.Linear(hidden_dim, hidden_dim)   # output\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout_ratio)\n",
        "\n",
        "        self.scale = torch.sqrt(torch.FloatTensor([self.head_dim])).to(device)\n",
        "\n",
        "    def forward(self, query, key, value, mask = None):\n",
        "\n",
        "        batch_size = query.shape[0]\n",
        "\n",
        "        # query : [batch_size, query_len, hidden_dim]\n",
        "        # key : [batch_size, key_len, hidden_dim]\n",
        "        # value : [batch_size, value_len, hidden_dim]\n",
        "\n",
        "        Q = self.fc_q(query)\n",
        "        K = self.fc_k(key)\n",
        "        V = self.fc_v(value)\n",
        "\n",
        "        # Q : [batch_size, query_len, hidden_dim]\n",
        "        # K : [batch_size, key_len, hidden_dim]\n",
        "        # V : [batch_size, value_len, hidden_dim]\n",
        "\n",
        "        # hidden_dim => n_heads X head_dim 형태로 변환\n",
        "        # n_heads(h)개의 서로 다른 어텐션(attention) 컨셉을 학습하도록 유도\n",
        "        Q = Q.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n",
        "        K = K.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n",
        "        V = V.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n",
        "\n",
        "        # Q : [batch_size, n_heads, query_len, head_dim]\n",
        "        # K : [batch_size, n_heads, key_len, head_dim]\n",
        "        # V : [batch_size, n_heads, value_len, head_dim]\n",
        "\n",
        "        # Attention Energy 계산\n",
        "        energy = torch.matmul(Q, K.permute(0, 1, 3, 2)) / self.scale\n",
        "\n",
        "        # energy : [batch_size, n_heads, query_len, key_len]\n",
        "\n",
        "        # 마스크(mask)를 사용하는 경우\n",
        "        if mask is not None:\n",
        "            # 마스크(mask) 값이 0인 부분을 -1e10으로 채우기\n",
        "            energy = energy.masked_fill(mask==0, -1e10)\n",
        "\n",
        "        # 어텐션(attention) 스코어 계산 : 각 단어에 대한 확률 값\n",
        "        attention = torch.softmax(energy, dim=-1)\n",
        "\n",
        "        # attention : [batch_size, n_heads, query_len, key_len]\n",
        "\n",
        "        # 여기에서 Scaled Dot-Product Attention을 계산\n",
        "        x = torch.matmul(self.dropout(attention), V)\n",
        "\n",
        "        # x : [batch_size, n_heads, query_len, head_dim]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9WsOTGye5dx6"
      },
      "source": [
        "# Layer Normalization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tss56pE3Kark"
      },
      "outputs": [],
      "source": [
        "class LayerNorm(nn.Module):\n",
        "    def __init__(self, d_model, eps=1e-8):\n",
        "        super(LayerNorm, self).__init__()\n",
        "        self.gamma = nn.Parameter(torch.ones(d_model))\n",
        "        self.beta = nn.Parameter(torch.zeros(d_model))\n",
        "        self.eps = eps\n",
        "\n",
        "    def forward(self, x):\n",
        "        mean = x.mean(-1, keepdim=True)\n",
        "        std = x.std(-1, keepdim=True)\n",
        "        return self.gamma * (X-mean) / (std + self.eps) + self.beta"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ShPnXMWKaOT"
      },
      "source": [
        "--------------------------"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Transformer_module.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
